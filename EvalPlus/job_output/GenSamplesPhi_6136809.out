
Generate samples...

[codecarbon INFO @ 21:51:10] [setup] RAM Tracking...
[codecarbon INFO @ 21:51:10] [setup] GPU Tracking...
[codecarbon INFO @ 21:51:10] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 21:51:10] [setup] CPU Tracking...
[codecarbon INFO @ 21:51:10] Tracking Intel CPU via RAPL interface
[codecarbon INFO @ 21:51:10] >>> Tracker's metadata:
[codecarbon INFO @ 21:51:10]   Platform system: Linux-4.18.0-372.80.1.el8_6.x86_64-x86_64-with-glibc2.28
[codecarbon INFO @ 21:51:10]   Python version: 3.11.5
[codecarbon INFO @ 21:51:10]   CodeCarbon version: 2.3.5
[codecarbon INFO @ 21:51:10]   Available RAM : 120.000 GB
[codecarbon INFO @ 21:51:10]   CPU count: 18
[codecarbon INFO @ 21:51:10]   CPU model: Intel(R) Xeon(R) Platinum 8360Y CPU @ 2.40GHz
[codecarbon INFO @ 21:51:10]   GPU count: 1
[codecarbon INFO @ 21:51:10]   GPU model: 1 x NVIDIA A100-SXM4-40GB
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.55s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:51:28] Energy consumed for RAM : 0.000188 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:51:28] Energy consumed for all GPUs : 0.000328 kWh. Total GPU Power : 78.80733976108088 W
[codecarbon INFO @ 21:51:28] Energy consumed for all CPUs : 0.001199 kWh. Total CPU Power : 287.59999953798734 W
[codecarbon INFO @ 21:51:28] 0.001715 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:51:43] Energy consumed for RAM : 0.000375 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:51:43] Energy consumed for all GPUs : 0.000750 kWh. Total GPU Power : 101.23128803760834 W
[codecarbon INFO @ 21:51:43] Energy consumed for all CPUs : 0.002399 kWh. Total CPU Power : 288.05601594178415 W
[codecarbon INFO @ 21:51:43] 0.003524 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:51:58] Energy consumed for RAM : 0.000562 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:51:58] Energy consumed for all GPUs : 0.001173 kWh. Total GPU Power : 101.6061550914101 W
[codecarbon INFO @ 21:51:58] Energy consumed for all CPUs : 0.003594 kWh. Total CPU Power : 286.695525075204 W
[codecarbon INFO @ 21:51:58] 0.005330 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:52:13] Energy consumed for RAM : 0.000750 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:52:13] Energy consumed for all GPUs : 0.001596 kWh. Total GPU Power : 101.46623153725241 W
[codecarbon INFO @ 21:52:13] Energy consumed for all CPUs : 0.004694 kWh. Total CPU Power : 264.1224106149049 W
[codecarbon INFO @ 21:52:13] 0.007040 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:52:28] Energy consumed for RAM : 0.000937 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:52:28] Energy consumed for all GPUs : 0.002016 kWh. Total GPU Power : 100.82153725299742 W
[codecarbon INFO @ 21:52:28] Energy consumed for all CPUs : 0.005797 kWh. Total CPU Power : 264.71249906243656 W
[codecarbon INFO @ 21:52:28] 0.008751 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:52:43] Energy consumed for RAM : 0.001125 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:52:43] Energy consumed for all GPUs : 0.002440 kWh. Total GPU Power : 101.72741679096919 W
[codecarbon INFO @ 21:52:43] Energy consumed for all CPUs : 0.006900 kWh. Total CPU Power : 264.6931597487451 W
[codecarbon INFO @ 21:52:43] 0.010465 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:52:58] Energy consumed for RAM : 0.001312 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:52:58] Energy consumed for all GPUs : 0.002863 kWh. Total GPU Power : 101.54281811873155 W
[codecarbon INFO @ 21:52:58] Energy consumed for all CPUs : 0.007999 kWh. Total CPU Power : 263.7204110212456 W
[codecarbon INFO @ 21:52:58] 0.012174 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:53:13] Energy consumed for RAM : 0.001500 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:53:13] Energy consumed for all GPUs : 0.003283 kWh. Total GPU Power : 100.81045838337215 W
[codecarbon INFO @ 21:53:13] Energy consumed for all CPUs : 0.009102 kWh. Total CPU Power : 264.7551278370509 W
[codecarbon INFO @ 21:53:13] 0.013885 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:53:28] Energy consumed for RAM : 0.001687 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:53:28] Energy consumed for all GPUs : 0.003708 kWh. Total GPU Power : 102.1423734072765 W
[codecarbon INFO @ 21:53:28] Energy consumed for all CPUs : 0.010197 kWh. Total CPU Power : 262.86693848929633 W
[codecarbon INFO @ 21:53:28] 0.015593 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:53:43] Energy consumed for RAM : 0.001875 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:53:43] Energy consumed for all GPUs : 0.004133 kWh. Total GPU Power : 101.97068000140438 W
[codecarbon INFO @ 21:53:43] Energy consumed for all CPUs : 0.011297 kWh. Total CPU Power : 263.91243296063755 W
[codecarbon INFO @ 21:53:43] 0.017305 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:53:58] Energy consumed for RAM : 0.002062 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:53:58] Energy consumed for all GPUs : 0.004559 kWh. Total GPU Power : 102.31097947998701 W
[codecarbon INFO @ 21:53:58] Energy consumed for all CPUs : 0.012416 kWh. Total CPU Power : 268.55436582234074 W
[codecarbon INFO @ 21:53:58] 0.019037 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:54:13] Energy consumed for RAM : 0.002250 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:54:13] Energy consumed for all GPUs : 0.004982 kWh. Total GPU Power : 101.40797703464358 W
[codecarbon INFO @ 21:54:13] Energy consumed for all CPUs : 0.013525 kWh. Total CPU Power : 266.2601229301093 W
[codecarbon INFO @ 21:54:13] 0.020757 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:54:28] Energy consumed for RAM : 0.002437 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:54:28] Energy consumed for all GPUs : 0.005406 kWh. Total GPU Power : 101.92739375896554 W
[codecarbon INFO @ 21:54:28] Energy consumed for all CPUs : 0.014634 kWh. Total CPU Power : 266.03951791354797 W
[codecarbon INFO @ 21:54:28] 0.022477 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:54:43] Energy consumed for RAM : 0.002625 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:54:43] Energy consumed for all GPUs : 0.005830 kWh. Total GPU Power : 101.70555553656261 W
[codecarbon INFO @ 21:54:43] Energy consumed for all CPUs : 0.015752 kWh. Total CPU Power : 268.43938755272893 W
[codecarbon INFO @ 21:54:43] 0.024207 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:54:58] Energy consumed for RAM : 0.002812 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:54:58] Energy consumed for all GPUs : 0.006254 kWh. Total GPU Power : 101.75393328018993 W
[codecarbon INFO @ 21:54:58] Energy consumed for all CPUs : 0.016862 kWh. Total CPU Power : 266.3942115676723 W
[codecarbon INFO @ 21:54:58] 0.025928 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:55:13] Energy consumed for RAM : 0.002999 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:55:13] Energy consumed for all GPUs : 0.006676 kWh. Total GPU Power : 101.30395034650064 W
[codecarbon INFO @ 21:55:13] Energy consumed for all CPUs : 0.017969 kWh. Total CPU Power : 265.71970553230176 W
[codecarbon INFO @ 21:55:13] 0.027645 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:55:28] Energy consumed for RAM : 0.003187 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:55:28] Energy consumed for all GPUs : 0.007101 kWh. Total GPU Power : 102.03058886128711 W
[codecarbon INFO @ 21:55:28] Energy consumed for all CPUs : 0.019073 kWh. Total CPU Power : 264.7600159899572 W
[codecarbon INFO @ 21:55:28] 0.029361 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:55:43] Energy consumed for RAM : 0.003374 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:55:43] Energy consumed for all GPUs : 0.007529 kWh. Total GPU Power : 102.69049761989747 W
[codecarbon INFO @ 21:55:43] Energy consumed for all CPUs : 0.020173 kWh. Total CPU Power : 264.0678010466716 W
[codecarbon INFO @ 21:55:43] 0.031076 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:55:58] Energy consumed for RAM : 0.003562 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:55:58] Energy consumed for all GPUs : 0.007956 kWh. Total GPU Power : 102.64149152601178 W
[codecarbon INFO @ 21:55:58] Energy consumed for all CPUs : 0.021282 kWh. Total CPU Power : 266.2443502997658 W
[codecarbon INFO @ 21:55:58] 0.032800 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:56:13] Energy consumed for RAM : 0.003749 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:56:13] Energy consumed for all GPUs : 0.008380 kWh. Total GPU Power : 101.62168599370098 W
[codecarbon INFO @ 21:56:13] Energy consumed for all CPUs : 0.022395 kWh. Total CPU Power : 266.9599241403207 W
[codecarbon INFO @ 21:56:13] 0.034523 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:56:28] Energy consumed for RAM : 0.003937 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:56:28] Energy consumed for all GPUs : 0.008807 kWh. Total GPU Power : 102.62888184204084 W
[codecarbon INFO @ 21:56:28] Energy consumed for all CPUs : 0.023498 kWh. Total CPU Power : 264.8842577679235 W
[codecarbon INFO @ 21:56:28] 0.036242 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:56:43] Energy consumed for RAM : 0.004124 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:56:43] Energy consumed for all GPUs : 0.009232 kWh. Total GPU Power : 101.84483721204829 W
[codecarbon INFO @ 21:56:43] Energy consumed for all CPUs : 0.024598 kWh. Total CPU Power : 263.86722654610713 W
[codecarbon INFO @ 21:56:43] 0.037953 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:56:58] Energy consumed for RAM : 0.004312 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:56:58] Energy consumed for all GPUs : 0.009659 kWh. Total GPU Power : 102.58830244802591 W
[codecarbon INFO @ 21:56:58] Energy consumed for all CPUs : 0.025713 kWh. Total CPU Power : 267.6173991093467 W
[codecarbon INFO @ 21:56:58] 0.039683 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:57:13] Energy consumed for RAM : 0.004499 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:57:13] Energy consumed for all GPUs : 0.010082 kWh. Total GPU Power : 101.6358863887154 W
[codecarbon INFO @ 21:57:13] Energy consumed for all CPUs : 0.026827 kWh. Total CPU Power : 267.39824913935206 W
[codecarbon INFO @ 21:57:13] 0.041408 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:57:28] Energy consumed for RAM : 0.004687 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:57:28] Energy consumed for all GPUs : 0.010509 kWh. Total GPU Power : 102.52377417828004 W
[codecarbon INFO @ 21:57:28] Energy consumed for all CPUs : 0.027938 kWh. Total CPU Power : 266.6802137542734 W
[codecarbon INFO @ 21:57:28] 0.043134 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:57:43] Energy consumed for RAM : 0.004874 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:57:43] Energy consumed for all GPUs : 0.010935 kWh. Total GPU Power : 102.16565385229417 W
[codecarbon INFO @ 21:57:43] Energy consumed for all CPUs : 0.029054 kWh. Total CPU Power : 267.74323328785607 W
[codecarbon INFO @ 21:57:43] 0.044863 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:57:58] Energy consumed for RAM : 0.005061 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:57:58] Energy consumed for all GPUs : 0.011361 kWh. Total GPU Power : 102.35885067519447 W
[codecarbon INFO @ 21:57:58] Energy consumed for all CPUs : 0.030164 kWh. Total CPU Power : 266.58333772594824 W
[codecarbon INFO @ 21:57:58] 0.046587 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:58:13] Energy consumed for RAM : 0.005249 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:58:13] Energy consumed for all GPUs : 0.011787 kWh. Total GPU Power : 102.2788717004813 W
[codecarbon INFO @ 21:58:13] Energy consumed for all CPUs : 0.031273 kWh. Total CPU Power : 266.1864438877597 W
[codecarbon INFO @ 21:58:13] 0.048310 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:58:28] Energy consumed for RAM : 0.005436 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:58:28] Energy consumed for all GPUs : 0.012214 kWh. Total GPU Power : 102.3253845432673 W
[codecarbon INFO @ 21:58:28] Energy consumed for all CPUs : 0.032391 kWh. Total CPU Power : 268.1072765682109 W
[codecarbon INFO @ 21:58:28] 0.050041 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:58:43] Energy consumed for RAM : 0.005624 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:58:43] Energy consumed for all GPUs : 0.012643 kWh. Total GPU Power : 102.97820820710159 W
[codecarbon INFO @ 21:58:43] Energy consumed for all CPUs : 0.033503 kWh. Total CPU Power : 266.9134164034559 W
[codecarbon INFO @ 21:58:43] 0.051769 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:58:58] Energy consumed for RAM : 0.005811 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:58:58] Energy consumed for all GPUs : 0.013070 kWh. Total GPU Power : 102.5543308767053 W
[codecarbon INFO @ 21:58:58] Energy consumed for all CPUs : 0.034603 kWh. Total CPU Power : 263.9786587219203 W
[codecarbon INFO @ 21:58:58] 0.053484 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:59:13] Energy consumed for RAM : 0.005999 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:59:13] Energy consumed for all GPUs : 0.013496 kWh. Total GPU Power : 102.23153831315656 W
[codecarbon INFO @ 21:59:13] Energy consumed for all CPUs : 0.035699 kWh. Total CPU Power : 263.16742703148816 W
[codecarbon INFO @ 21:59:13] 0.055194 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:59:28] Energy consumed for RAM : 0.006186 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:59:28] Energy consumed for all GPUs : 0.013927 kWh. Total GPU Power : 103.419488032798 W
[codecarbon INFO @ 21:59:28] Energy consumed for all CPUs : 0.036814 kWh. Total CPU Power : 267.69043646900786 W
[codecarbon INFO @ 21:59:28] 0.056927 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:59:43] Energy consumed for RAM : 0.006374 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:59:43] Energy consumed for all GPUs : 0.014352 kWh. Total GPU Power : 102.10363147146093 W
[codecarbon INFO @ 21:59:43] Energy consumed for all CPUs : 0.037919 kWh. Total CPU Power : 265.0199654620791 W
[codecarbon INFO @ 21:59:43] 0.058644 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 21:59:58] Energy consumed for RAM : 0.006561 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 21:59:58] Energy consumed for all GPUs : 0.014778 kWh. Total GPU Power : 102.356082726324 W
[codecarbon INFO @ 21:59:58] Energy consumed for all CPUs : 0.039026 kWh. Total CPU Power : 265.8017784400207 W
[codecarbon INFO @ 21:59:58] 0.060366 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 22:00:13] Energy consumed for RAM : 0.006749 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 22:00:13] Energy consumed for all GPUs : 0.015201 kWh. Total GPU Power : 101.41287573645232 W
[codecarbon INFO @ 22:00:13] Energy consumed for all CPUs : 0.040131 kWh. Total CPU Power : 265.2260853659461 W
[codecarbon INFO @ 22:00:13] 0.062081 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 22:00:28] Energy consumed for RAM : 0.006936 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 22:00:28] Energy consumed for all GPUs : 0.015627 kWh. Total GPU Power : 102.35112922632017 W
[codecarbon INFO @ 22:00:28] Energy consumed for all CPUs : 0.041232 kWh. Total CPU Power : 264.2708313966658 W
[codecarbon INFO @ 22:00:28] 0.063796 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 22:00:43] Energy consumed for RAM : 0.007124 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 22:00:43] Energy consumed for all GPUs : 0.016053 kWh. Total GPU Power : 102.2554065578752 W
[codecarbon INFO @ 22:00:43] Energy consumed for all CPUs : 0.042344 kWh. Total CPU Power : 266.8050618678584 W
[codecarbon INFO @ 22:00:43] 0.065521 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 22:00:58] Energy consumed for RAM : 0.007311 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 22:00:58] Energy consumed for all GPUs : 0.016480 kWh. Total GPU Power : 102.50991457755839 W
[codecarbon INFO @ 22:00:58] Energy consumed for all CPUs : 0.043456 kWh. Total CPU Power : 266.7562187060348 W
[codecarbon INFO @ 22:00:58] 0.067247 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 22:01:13] Energy consumed for RAM : 0.007498 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 22:01:13] Energy consumed for all GPUs : 0.016904 kWh. Total GPU Power : 101.82171157091311 W
[codecarbon INFO @ 22:01:13] Energy consumed for all CPUs : 0.044576 kWh. Total CPU Power : 268.84520171170766 W
[codecarbon INFO @ 22:01:13] 0.068979 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 22:01:28] Energy consumed for RAM : 0.007686 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 22:01:28] Energy consumed for all GPUs : 0.017331 kWh. Total GPU Power : 102.48616863899332 W
[codecarbon INFO @ 22:01:28] Energy consumed for all CPUs : 0.045732 kWh. Total CPU Power : 277.55737876574415 W
[codecarbon INFO @ 22:01:28] 0.070750 kWh of electricity used since the beginning.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[codecarbon INFO @ 22:01:43] Energy consumed for RAM : 0.007869 kWh. RAM Power : 45.0 W
[codecarbon INFO @ 22:01:43] Energy consumed for all GPUs : 0.017747 kWh. Total GPU Power : 102.03379843083395 W
[codecarbon INFO @ 22:01:43] Energy consumed for all CPUs : 0.046834 kWh. Total CPU Power : 270.2252729873915 W
[codecarbon INFO @ 22:01:43] 0.072451 kWh of electricity used since the beginning.
0.02566988108064019

Samples generated.
Proceeding to sanitizing..

0it [00:00, ?it/s]/home/pdereus/.conda/envs/thesis/lib/python3.11/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.
  warn("{} is deprecated. Use {} instead.".format(old, new), FutureWarning)
Sanitized: HumanEval/0 (line 1 in samples_phi.jsonl)
Sanitized: HumanEval/1 (line 2 in samples_phi.jsonl)
Sanitized: HumanEval/2 (line 3 in samples_phi.jsonl)
Sanitized: HumanEval/3 (line 4 in samples_phi.jsonl)
Sanitized: HumanEval/4 (line 5 in samples_phi.jsonl)
Sanitized: HumanEval/5 (line 6 in samples_phi.jsonl)
Sanitized: HumanEval/6 (line 7 in samples_phi.jsonl)
Sanitized: HumanEval/7 (line 8 in samples_phi.jsonl)
Sanitized: HumanEval/8 (line 9 in samples_phi.jsonl)
Sanitized: HumanEval/9 (line 10 in samples_phi.jsonl)
Sanitized: HumanEval/10 (line 11 in samples_phi.jsonl)
Sanitized: HumanEval/11 (line 12 in samples_phi.jsonl)
Sanitized: HumanEval/12 (line 13 in samples_phi.jsonl)
Sanitized: HumanEval/13 (line 14 in samples_phi.jsonl)
Sanitized: HumanEval/14 (line 15 in samples_phi.jsonl)
Sanitized: HumanEval/15 (line 16 in samples_phi.jsonl)
Sanitized: HumanEval/16 (line 17 in samples_phi.jsonl)
Sanitized: HumanEval/17 (line 18 in samples_phi.jsonl)
Sanitized: HumanEval/18 (line 19 in samples_phi.jsonl)
Sanitized: HumanEval/19 (line 20 in samples_phi.jsonl)
Sanitized: HumanEval/20 (line 21 in samples_phi.jsonl)
Sanitized: HumanEval/21 (line 22 in samples_phi.jsonl)
Sanitized: HumanEval/22 (line 23 in samples_phi.jsonl)
Sanitized: HumanEval/23 (line 24 in samples_phi.jsonl)
Sanitized: HumanEval/24 (line 25 in samples_phi.jsonl)
Sanitized: HumanEval/25 (line 26 in samples_phi.jsonl)
Sanitized: HumanEval/26 (line 27 in samples_phi.jsonl)
Sanitized: HumanEval/27 (line 28 in samples_phi.jsonl)
Sanitized: HumanEval/28 (line 29 in samples_phi.jsonl)
Sanitized: HumanEval/29 (line 30 in samples_phi.jsonl)
Sanitized: HumanEval/30 (line 31 in samples_phi.jsonl)
Sanitized: HumanEval/31 (line 32 in samples_phi.jsonl)
Sanitized: HumanEval/32 (line 33 in samples_phi.jsonl)
Sanitized: HumanEval/33 (line 34 in samples_phi.jsonl)
Sanitized: HumanEval/34 (line 35 in samples_phi.jsonl)
Sanitized: HumanEval/35 (line 36 in samples_phi.jsonl)
Sanitized: HumanEval/36 (line 37 in samples_phi.jsonl)
Sanitized: HumanEval/37 (line 38 in samples_phi.jsonl)
Sanitized: HumanEval/38 (line 39 in samples_phi.jsonl)
Sanitized: HumanEval/39 (line 40 in samples_phi.jsonl)
Sanitized: HumanEval/40 (line 41 in samples_phi.jsonl)
Sanitized: HumanEval/41 (line 42 in samples_phi.jsonl)
Sanitized: HumanEval/42 (line 43 in samples_phi.jsonl)
Sanitized: HumanEval/43 (line 44 in samples_phi.jsonl)
Sanitized: HumanEval/44 (line 45 in samples_phi.jsonl)
Sanitized: HumanEval/45 (line 46 in samples_phi.jsonl)
Sanitized: HumanEval/46 (line 47 in samples_phi.jsonl)
Sanitized: HumanEval/47 (line 48 in samples_phi.jsonl)
Sanitized: HumanEval/48 (line 49 in samples_phi.jsonl)
Sanitized: HumanEval/49 (line 50 in samples_phi.jsonl)
Sanitized: HumanEval/50 (line 51 in samples_phi.jsonl)
Sanitized: HumanEval/51 (line 52 in samples_phi.jsonl)
Sanitized: HumanEval/52 (line 53 in samples_phi.jsonl)
Sanitized: HumanEval/53 (line 54 in samples_phi.jsonl)
Sanitized: HumanEval/54 (line 55 in samples_phi.jsonl)
Sanitized: HumanEval/55 (line 56 in samples_phi.jsonl)
Sanitized: HumanEval/56 (line 57 in samples_phi.jsonl)
Sanitized: HumanEval/57 (line 58 in samples_phi.jsonl)
Sanitized: HumanEval/58 (line 59 in samples_phi.jsonl)
Sanitized: HumanEval/59 (line 60 in samples_phi.jsonl)
Sanitized: HumanEval/60 (line 61 in samples_phi.jsonl)
Sanitized: HumanEval/61 (line 62 in samples_phi.jsonl)
Sanitized: HumanEval/62 (line 63 in samples_phi.jsonl)
Sanitized: HumanEval/63 (line 64 in samples_phi.jsonl)
Sanitized: HumanEval/64 (line 65 in samples_phi.jsonl)
Sanitized: HumanEval/65 (line 66 in samples_phi.jsonl)
Sanitized: HumanEval/66 (line 67 in samples_phi.jsonl)
Sanitized: HumanEval/67 (line 68 in samples_phi.jsonl)
Sanitized: HumanEval/68 (line 69 in samples_phi.jsonl)
Sanitized: HumanEval/69 (line 70 in samples_phi.jsonl)
Sanitized: HumanEval/70 (line 71 in samples_phi.jsonl)
Sanitized: HumanEval/71 (line 72 in samples_phi.jsonl)
Sanitized: HumanEval/72 (line 73 in samples_phi.jsonl)
Sanitized: HumanEval/73 (line 74 in samples_phi.jsonl)
Sanitized: HumanEval/74 (line 75 in samples_phi.jsonl)
Sanitized: HumanEval/75 (line 76 in samples_phi.jsonl)
Sanitized: HumanEval/76 (line 77 in samples_phi.jsonl)
Sanitized: HumanEval/77 (line 78 in samples_phi.jsonl)
Sanitized: HumanEval/78 (line 79 in samples_phi.jsonl)
Sanitized: HumanEval/79 (line 80 in samples_phi.jsonl)
Sanitized: HumanEval/80 (line 81 in samples_phi.jsonl)
Sanitized: HumanEval/81 (line 82 in samples_phi.jsonl)
Sanitized: HumanEval/82 (line 83 in samples_phi.jsonl)
Sanitized: HumanEval/83 (line 84 in samples_phi.jsonl)
Sanitized: HumanEval/84 (line 85 in samples_phi.jsonl)
Sanitized: HumanEval/85 (line 86 in samples_phi.jsonl)
Sanitized: HumanEval/86 (line 87 in samples_phi.jsonl)
Sanitized: HumanEval/87 (line 88 in samples_phi.jsonl)
Sanitized: HumanEval/88 (line 89 in samples_phi.jsonl)
Sanitized: HumanEval/89 (line 90 in samples_phi.jsonl)
Sanitized: HumanEval/90 (line 91 in samples_phi.jsonl)
Sanitized: HumanEval/91 (line 92 in samples_phi.jsonl)
Sanitized: HumanEval/92 (line 93 in samples_phi.jsonl)
Sanitized: HumanEval/93 (line 94 in samples_phi.jsonl)
Sanitized: HumanEval/94 (line 95 in samples_phi.jsonl)
Sanitized: HumanEval/95 (line 96 in samples_phi.jsonl)
Sanitized: HumanEval/96 (line 97 in samples_phi.jsonl)
Sanitized: HumanEval/97 (line 98 in samples_phi.jsonl)
Sanitized: HumanEval/98 (line 99 in samples_phi.jsonl)
Sanitized: HumanEval/99 (line 100 in samples_phi.jsonl)
Sanitized: HumanEval/100 (line 101 in samples_phi.jsonl)
Sanitized: HumanEval/101 (line 102 in samples_phi.jsonl)
Sanitized: HumanEval/102 (line 103 in samples_phi.jsonl)
Sanitized: HumanEval/103 (line 104 in samples_phi.jsonl)
Sanitized: HumanEval/104 (line 105 in samples_phi.jsonl)
Sanitized: HumanEval/105 (line 106 in samples_phi.jsonl)
Sanitized: HumanEval/106 (line 107 in samples_phi.jsonl)
Sanitized: HumanEval/107 (line 108 in samples_phi.jsonl)
Sanitized: HumanEval/108 (line 109 in samples_phi.jsonl)
Sanitized: HumanEval/109 (line 110 in samples_phi.jsonl)
Sanitized: HumanEval/110 (line 111 in samples_phi.jsonl)
Sanitized: HumanEval/111 (line 112 in samples_phi.jsonl)
Sanitized: HumanEval/112 (line 113 in samples_phi.jsonl)
Sanitized: HumanEval/113 (line 114 in samples_phi.jsonl)
Sanitized: HumanEval/114 (line 115 in samples_phi.jsonl)
Sanitized: HumanEval/115 (line 116 in samples_phi.jsonl)
Sanitized: HumanEval/116 (line 117 in samples_phi.jsonl)
Sanitized: HumanEval/117 (line 118 in samples_phi.jsonl)
Sanitized: HumanEval/118 (line 119 in samples_phi.jsonl)
Sanitized: HumanEval/119 (line 120 in samples_phi.jsonl)
Sanitized: HumanEval/120 (line 121 in samples_phi.jsonl)
Sanitized: HumanEval/121 (line 122 in samples_phi.jsonl)
Sanitized: HumanEval/122 (line 123 in samples_phi.jsonl)
Sanitized: HumanEval/123 (line 124 in samples_phi.jsonl)
Sanitized: HumanEval/124 (line 125 in samples_phi.jsonl)
Sanitized: HumanEval/125 (line 126 in samples_phi.jsonl)
Sanitized: HumanEval/126 (line 127 in samples_phi.jsonl)
Sanitized: HumanEval/127 (line 128 in samples_phi.jsonl)
Sanitized: HumanEval/128 (line 129 in samples_phi.jsonl)
Sanitized: HumanEval/129 (line 130 in samples_phi.jsonl)
Sanitized: HumanEval/130 (line 131 in samples_phi.jsonl)
Sanitized: HumanEval/131 (line 132 in samples_phi.jsonl)
Sanitized: HumanEval/132 (line 133 in samples_phi.jsonl)
Sanitized: HumanEval/133 (line 134 in samples_phi.jsonl)
Sanitized: HumanEval/134 (line 135 in samples_phi.jsonl)
Sanitized: HumanEval/135 (line 136 in samples_phi.jsonl)
Sanitized: HumanEval/136 (line 137 in samples_phi.jsonl)
Sanitized: HumanEval/137 (line 138 in samples_phi.jsonl)
Sanitized: HumanEval/138 (line 139 in samples_phi.jsonl)
Sanitized: HumanEval/139 (line 140 in samples_phi.jsonl)
Sanitized: HumanEval/140 (line 141 in samples_phi.jsonl)
Sanitized: HumanEval/141 (line 142 in samples_phi.jsonl)
Sanitized: HumanEval/142 (line 143 in samples_phi.jsonl)
Sanitized: HumanEval/143 (line 144 in samples_phi.jsonl)
Sanitized: HumanEval/144 (line 145 in samples_phi.jsonl)
Sanitized: HumanEval/145 (line 146 in samples_phi.jsonl)
Sanitized: HumanEval/146 (line 147 in samples_phi.jsonl)
16it [00:00, 157.23it/s]34it [00:00, 168.79it/s]53it [00:00, 178.13it/s]72it [00:00, 181.89it/s]92it [00:00, 187.94it/s]111it [00:00, 181.74it/s]130it [00:00, 178.62it/s]148it [00:00, 175.71it/s]164it [00:00, 179.77it/s]
Sanitized: HumanEval/147 (line 148 in samples_phi.jsonl)
Sanitized: HumanEval/148 (line 149 in samples_phi.jsonl)
Sanitized: HumanEval/149 (line 150 in samples_phi.jsonl)
Sanitized: HumanEval/150 (line 151 in samples_phi.jsonl)
Sanitized: HumanEval/151 (line 152 in samples_phi.jsonl)
Sanitized: HumanEval/152 (line 153 in samples_phi.jsonl)
Sanitized: HumanEval/153 (line 154 in samples_phi.jsonl)
Sanitized: HumanEval/154 (line 155 in samples_phi.jsonl)
Sanitized: HumanEval/155 (line 156 in samples_phi.jsonl)
Sanitized: HumanEval/156 (line 157 in samples_phi.jsonl)
Sanitized: HumanEval/157 (line 158 in samples_phi.jsonl)
Sanitized: HumanEval/158 (line 159 in samples_phi.jsonl)
Sanitized: HumanEval/159 (line 160 in samples_phi.jsonl)
Sanitized: HumanEval/160 (line 161 in samples_phi.jsonl)
Sanitized: HumanEval/161 (line 162 in samples_phi.jsonl)
Sanitized: HumanEval/162 (line 163 in samples_phi.jsonl)
Sanitized: HumanEval/163 (line 164 in samples_phi.jsonl)
Sanitized 164 out of 164 files.
Check the sanitized files at samples_phi-sanitized.jsonl

Evaluate samples without sanitize...

Load from ground-truth from /home/pdereus/.cache/evalplus/84f4b93a1270b492e4c54d5212da7a5b.pkl
Reading samples...
0it [00:00, ?it/s]1it [00:00,  1.99it/s]164it [00:00, 305.67it/s]
  0%|          | 0/164 [00:00<?, ?it/s] 10%|▉         | 16/164 [00:00<00:02, 60.94it/s] 29%|██▉       | 48/164 [00:00<00:00, 149.46it/s] 54%|█████▍    | 89/164 [00:00<00:00, 235.23it/s] 73%|███████▎  | 119/164 [00:00<00:00, 120.44it/s] 85%|████████▌ | 140/164 [00:01<00:00, 121.81it/s] 99%|█████████▉| 163/164 [00:01<00:00, 91.21it/s] 100%|██████████| 164/164 [00:02<00:00, 71.61it/s]
humaneval (base tests)
pass@1:	0.049
humaneval+ (base + extra tests)
pass@1:	0.043

Evaluate samples with sanitize...

Load from ground-truth from /home/pdereus/.cache/evalplus/84f4b93a1270b492e4c54d5212da7a5b.pkl
Reading samples...
0it [00:00, ?it/s]1it [00:00,  4.86it/s]164it [00:00, 681.08it/s]
  0%|          | 0/164 [00:00<?, ?it/s] 10%|▉         | 16/164 [00:00<00:02, 59.19it/s] 21%|██▏       | 35/164 [00:00<00:01, 102.81it/s] 34%|███▍      | 56/164 [00:00<00:00, 136.27it/s] 45%|████▍     | 73/164 [00:00<00:00, 140.41it/s] 54%|█████▍    | 89/164 [00:00<00:00, 140.67it/s] 64%|██████▍   | 105/164 [00:01<00:00, 76.42it/s] 77%|███████▋  | 126/164 [00:01<00:00, 95.54it/s] 94%|█████████▍| 154/164 [00:01<00:00, 131.65it/s]100%|██████████| 164/164 [00:02<00:00, 56.55it/s] 
humaneval (base tests)
pass@1:	0.451
humaneval+ (base + extra tests)
pass@1:	0.409

JOB STATISTICS
==============
Job ID: 6136809
Cluster: snellius
User/Group: pdereus/pdereus
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:11:08
CPU Efficiency: 4.66% of 03:59:06 core-walltime
Job Wall-clock time: 00:13:17
Memory Utilized: 1.16 GB
Memory Efficiency: 0.96% of 120.00 GB
